所有分布式几乎都用到的相似的设计: 分布式天下归一
  Chubby（Paxos算法）核心的多数同意，很多分布式架构 kaffka 等都用到了的Quorum机制
  Chubby服务端和客户端（包括缓存），数据同步类似 Eureka 的服务端和客户端
  Chubby底层日志持久化类似 Hadoop 的Editlog和fsImage，方便宕机重启后快速恢复

1: 分布式一致性: 
  种类: 保证数据一致性会因为阻塞降低性能，提高性能必然会降低一致性，所以对于一个系统，性能和一致性需要平衡。一般一致性有三种：
    强一致性: 类似Kafka等Patition的leader和Replica之间的数据写确认（全部replica确认写成功的情况）
    弱一致性: 类似Mysql主从同步，会有一定延迟，不承诺具体多久，但一般是秒级。
    最终一致性: 类似通过kafka中间件解耦的业务数据在各模块之间一致性的情况。

  1.1: 分布式特点: 
    *对等性
        副本（Replica）: 是分布式系统中对数据和服务提供的一种冗余方式。
    *并发性
    *缺乏全局时钟
    *故障总会发生

  1.2: 分布式环境问题: 
    *通讯异常
        延迟: 现在计算机体系结构中，单机内存访问延迟在纳秒级（10ns左右），而正常的网络通讯延迟在0.1-1ms左右（105—106倍）
        副本（Replica）: 是分布式系统中对数据和服务提供的一种冗余方式。
    *网络分区
    *三态
        三态: 成功、失败、超时
    *节点故障

  1.3: 分布式理论: # ACID -> 分布式事务 -> CAP/BASE
    - ACID: 原子、一致（执行一半宕机已执行的数据不写入）、隔离（4种隔离级别-脏读／幻读）、持久
    - CAP: 2000年加州大学伯克利分校教授，一个分布式系统不可能同时满足一致性(C)、可用性(A)、分区容错性(P)
            可用性: 系统提供的服务在有限的时间内返回结果，类似 ZK(CP) 和 Eureka(AP) 理念的不同。
                  # google一般在0.3秒，HIVE一般20-30s（时间跨度较大时可能几分钟）
            分区容错性: 指在出现网络分区故障时，可以提供一致性、可用的服务
    - BASE: 最终一致性，eBay架构师提出。
            基本可用: 响应时间稍加增长、功能损失（部分购物者被引导到降级页面）
            软状态: 指中间状态、且其存在不影响整体可用，不同微服务数据同步存在延迟
            最终一致性: 

  1.4: 分布式提高性能措施: 
    副本复制
    Master选举


2: 一致性协议(算法): 
  2.1: 2PC: 提交事务请求、执行事务请求 
       简单、实现方便；
       同步阻塞、无限期等待（可以通过超时限制但可能会脑裂）、脑裂（协调节点的单点故障导致部分提交、部分不提交导致数据不一致）

  2.2: 3PC: CanCommit、PreCommit、DoCommit
       协调节点的单点故障后可以达成一致；
       网络分区后会造成数据不一致

  总结: 其实2PC和3PC都无法解决脑裂问题，即使是3PC，当第三阶段脑裂，而一部分server在第三阶段收到rollback指令，其余的默认还是会docommit，而Paxos
       中提出了一个超过半数的概念，就可以解决这个问题。 
  # 通常一个分布式一致性算法都需要用到一个特殊的机制：Quorum，指某一数据项值选定过程中需要集群中过半的机器达成一致。因此’Quorum机制‘也成‘过半机制’

  2.3: Paxos: 拜占庭将军问题有两个核心：消息被篡改、一致性，但一般分布式系统都是在局域网内，所以假设消息不会被篡改。Lamport在1990提出该算法，并
              在论文中设想了一个场景来解释原理：古希腊Paxos小岛上的议会投票。 # 因不愿修改论文被某杂志延迟发表
              核心是一致性算法，解决的问题:
                - 被提出的提案才能被选中
                - 只能有一个值被选中
                - 如果某个进程认为一个提案被选中，那么该提案是所有人都认可的那个
              涉及到的对象:
                - Proposer：提议人
                - Acceptor：承兑人，Acceptor可以忽略任何请求（包括Prepare请求和Accept请求）而不用担心破坏算法的安全性
                - Learner：学习者
                - Proposal：提案有一个编号和具体提案内容组成的[M,V]
              过程:
                - Proposer发送编号Mx，如果Acceptor接受的M小于Mx（比如M0...Mx-1），那么把Mx作为response返回，且不在接受小于x的提案。
                - Proposer收到半数以上Mx返回，就发送[Mx,Vx]给所有Acceptor，任意一个Acceptor没有收到编号大于x的提案，就通过[Mx,Vx]的提案。
                注：
                  *第一个阶段可能会发生死循环，所以只有主Proposer提议发起提案
                  *提案通过后Proposer发给部分Learner组成的集合，再由他们通知所有Learner

       Paxos NB的应用1 -- Google Chubby: 
                大名鼎鼎的分布式所服务，GFS（2003）、BigTable（2006）等大型系统都用它来解决分布式协作、元数据存储和master选举等和分布式锁相关
              的问题。最典型的应用场景就是集群服务中的master选举。
                Chubby是一个面向松耦合分布式系统的锁服务，通常用于为一个由大量小型计算机构成的松耦合分布式系统提供高可用的分布式锁服务。（一个分
              布式锁服务的目的是允许他的客户端进程同步彼此的操作，并对所处环境的基本信息达成一致），Chubby提供了粗粒度的服务，客户端接口设计的类
              似Unix的文件系统结构，可以操作文件节点（小文件）和文件节点的锁，而开发人员直接调用客户端接口即可实现进程间粗粒度的同步控制，从而保
              证分布式数据的一致性。

              # MapReduce很多人都知道，但关于Chubyy似乎熟悉它的就非常有限，这倒不奇怪，因为MapReduce是一个针对开发人员的ProgrammingModel，
              # 自然会有很多人去学习它，而Chubby更多的是一种为了实现MapReduce或者Bigtable而构建的内部的工具，对于开发人员来说基本上是透明的。
              
              # Chubby首先是一个分布式的文件系统。Chubby能够提供机制使得client可以在Chubby service上创建文件和执行一些文件的基本操作。说它是
              # 分布式的文件系统，是因为一个Chubby cell是一个分布式的系统，一般包含了5台机器，整个文件系统是部署在这5台机器上的。但是，从更高一
              # 点的语义层面上，Chubby是一个 lock service，一个针对松耦合的分布式系统的lock service。所谓lock service，就是这个service能够
              # 提供开发人员经常用的“锁”“解锁”功能。通过Chubby，一个分布式系统中的上千个client都能够对于某项资源进行“加锁”，“解锁”。那么，Chubby
              # 是怎样实现这样的“锁”功能的？就是通过文件。Chubby中的“锁”就是文件，在上例中，创建文件其实就是进行“加锁”操作，创建文件成功的那个
              # server其实就是抢占到了“锁”。用户通过打开、关闭和读取文件，获取共享锁或者独占锁；并且通过通信机制，向用户发送更新信息。综上所述，
              # Chubby是一个lock service，通过这个lock service可以解决分布式中的一致性问题，而这个lock service的实现是一个分布式的文件系统。
       
       Google Chubby应用场景／设计目标（侵入小、数据发布／订阅、高可靠高可用、粗粒度等）: 
              客户端一旦获取锁后会长时间持有(几个小时或者几天)，即使短暂宕机也不会丢失持有状态，这也就是所说的粗粒度。master获取锁后，会把master
              信息写入该文件节点，其他节点失去该节点的写权限，但是可以读取该节点，以获取master信息。可以支撑成百上千个客户端节点的监视、读取。服务
              端的信息变化是通过事件的形式通知所有订阅的客户端，而不是通过客户端不断轮训。
       
       Google Chubby技术架构: 
              服务端和客户端组成，客户端通过RPC调用与服务端通讯。一般使用5台机器构成一个集群，通过Paxos协议选举出一个master，在一个租期内Chubby
              会保证该机器是唯一的master，到期后可以续租。如果Master对应的机器宕机，租期到期后会重新选举，一般花费几秒钟。
              客户端向服务端注册事件通知，如果服务端节点发生变化，进行回调，类似EurekaServer和EurekaClient，且更相似的是，在ChubbyClient会有
              文件内容和原数据信息的缓存。（服务端和客户端缓存一致性策略：客户端租期(一般12s) + 服务端变更通知）
              会话:
                客户端和服务端通过TCP连接+心跳机制保持会话的活性，以使会话周期得到延续。
              层级结构:
                容错日志系统：低层，通过Poxos选举Master进行日志复制
                容错数据库：中层，利用BTree进行日志存储 # 采用类似hadoop的editLog和fsImage来定期清理、存储日志到磁盘，宕机重启后可迅速恢复数据
                Chubby服务：最上层，提供分布式锁和小文件存储服务

       Paxos NB的应用2 -- Hypertable: 
                根据Google的BigTable相关论文，使用C++开发的开源、高性能、可伸缩的数据库，与Hbase类似，用于构建一个分布式海量数据的高并发数据库，
              不支持事务、关联查询，少量数据时查询效率不如传统的关系型数据库，但支持高并发、海量、随意扩容、节点失效后高可用。


3: ZooKeeper: 
  2011年成为Apache顶级项目，从Apache Hadoop子项目发展来的，雅虎创建，是Google Chubby的开源实现。提供高效、可靠的分布式协调服务，如统一命名、配
  置管理、分布式锁。没有直接采用Paxos协议，用的是 ZAB 一致性协议（ZooKeeper Atomic Broadcast）。


















