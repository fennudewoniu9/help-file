# 参考：https://www.cnblogs.com/jshen/p/7682502.html
#      https://blog.csdn.net/xlgen157387/article/details/53976153
#      https://blog.csdn.net/mingover/article/details/71108852
01: 水平拆分、垂直拆分:
    所谓垂直分区指的是可以根据业务自身的不同，将原本冗余在一个数据库内的业务表拆散，将数据分别存储在不同的数据库中，同时仍然保持 Master/Salve模式。
    所谓水平分区指的是将一个业务表拆分成多个子表，比如 user_table0 、 user_table1 、 user_table2 。
    - 好处:
    Sharding的基本思想就要把一个数据库切分成多个部分放到不同的数据库(server)上，从而缓解单一数据库的性能问题。
    实践中不管水平还是垂直，会同时分库分表。原因：只有分库可以解决单台数据库的并发访问压力问题；分表（同一个数据库）可以解决单表海量数据的查询性能
问题，但是无法给数据库的并发操作带来效率上的提高，因为分表的实质还是在一个数据库上进行的操作，很容易受数据库IO性能的限制。但是如果只是数据量大，只
分库也行（一个库里表只有一张）。
    - 方式:
    不太严格的讲，对于海量数据的数据库，如果是因为表多而数据多，这时候适合使用垂直切分，即把关系紧密（比如同一模块）的表切分出来放在一个server上
。如果表不多，但每张表的数据非常多，这时适合水平切分，即把表的数据按某种规则（比如按ID散列）切分到多个数据库(server)上。但，现实中更多是这两种情
况混杂在一起，这时候需要根据实际情况做出选择，可能会综合使用垂直与水平切分，从而将原有数据库切分成类似矩阵一样可以无限扩充的数据库(server)阵列。
    - 分库+分表策略: # 如果只有一中情况（分库或者分表）的话，实现策略只需要一次路由计算。
      一种常见的路由策略如下：
        1.中间变量　＝ user_id%（库数量*每个库的表数量）;
        2.库序号　＝　取整（中间变量／每个库的表数量）;
        3.表序号　＝　中间变量％每个库的表数量;
      例如：数据库有256 个，每一个库中有1024个数据表，用户的user_id＝262145，按照上述的路由策略，可得：
        1.中间变量　＝ 262145%（256*1024）= 1;
        2.库序号　＝　取整（1／1024）= 0;
        3.表序号　＝　1％1024 = 1;
      这样的话，对于user_id＝262145，将被路由到第０个数据库的第１个表中。
    - 共同问题: 
      * 引入分布式事务的问题
      * 跨节点 Join 的问题
      * 跨节点合并排序分页问题
      * 多数据源管理问题

    - 原则:
      * 能不切分尽量不要切分
      * 如果要切分一定要选择合适的切分规则，提前规划好
      * 数据切分尽量通过数据冗余或表分组（Table Group）来降低跨库 Join 的可能
      * 由于数据库中间件对数据 Join 实现的优劣难以把握，而且实现高性能难度极大，业务读取尽量少使用多表 Join
    - 方法: # 可以这样理解：微服务不同模块使用不同的数据库；当两种切分方式都存在时可以理解成，在此基础上再进一步细化拆分粒度，保证每一个微服务DB
           # 只有一个主表需要和其他模块打交道。实际上一单采用微服务模式，就不存在垂直且分的概念，因为微服务本身就是按功能切分的。
    当同时进行垂直和水平切分时，切分策略会发生一些微妙的变化。比如：在只考虑垂直切分的时候，被划分到一起的表之间可以保持任意的关联关系，因此你可以
按“功能模块”划分表格，但是一旦引入水平切分之后，表间关联关系就会受到很大的制约，通常只能允许一个主表（以该表ID进行散列的表）和其多个次表之间保留关
联关系，也就是说：当同时进行垂直和水平切分时，在垂直方向上的切分将不再以“功能模块”进行划分，而是需要更加细粒度的垂直切分。
    - 思想:  # 在细粒度的拆分基础上降低数据源管理成本
    而这个粒度与 领域驱动设计 中的“聚合”概念不谋而合，甚至可以说是完全一致，每个shard的主表正是一个聚合中的聚合根！这样切分下来你会发现数据库被切
分地过于分散了（shard的数量会比较多，但是shard里的表却不多），为了避免管理过多的数据源，充分利用每一个数据库服务器的资源，可以考虑将业务上相近，并
且具有相近数据增长速率（主表数据量在同一数量级上）的两个或多个shard放到同一个数据源里，每个shard依然是独立的，它们有各自的主表，并使用各自主表ID进
行散列，不同的只是它们的散列取模（即节点数量）必需是一致的.


02: 数据库扩容:
    业务拆分、主从复制，数据库分库与分表；以下内容都是基于分库分表。


03: 高性能数据库:
    集群控制，集群的负载均衡，灾难恢复，故障自动切换，事务管理等。

总之:
    路漫漫其修远兮，吾将上下而求索。
    前方道路美好而光明，2018年新征程，不泄步！





1、2: 如何实现高效主键、及数据库扩容:
   * 分库方式:
     - 根据数值范围，比如用户Id为1-9999的记录分到第一个库，10000-20000的分到第二个库，以此类推。
     - 根据数值取模（基因拆分法），比如用户Id mod n，余数为0的记录放到第一个库，余数为1的放到第二个库，以此类推。
       该方法类似基因拆分法，但如果按照用户主Id取模分库，当改Id的订单爆多时，某些库的记录数会特别多，对于这些超级Id，需要提供单独库来存储记录。
     - 两种方法的比较：
      ----评价指标--------------------按照数值范围分库----------------------------------------按照数值Mod分库--------------------
      ----库数量-------------前期数目比较小，可以随用户/业务按需增长-------------------前期即根据mode因子确定库数量，数目一般比较大----
      ----访问性能--------前期库数量小，全库查询消耗资源少，单库查询性能略差---------前期库数量大，全库查询消耗资源多，单库查询性能略好----
      ----调整库数量----比较容易，一般只需为新用户增加库，老库拆分也只影响单个库---------困难，改变mod因子导致数据在所有库之间迁移--------
      ----数据热点-------------新旧用户购物频率有差异，有数据热点问题----------------------新旧用户均匀到分布到各个库，无热点----------
      # 实践中，为了处理简单，选择mod分库的比较多。同时二次分库时，为了数据迁移方便，一般是按倍数增加，比如初始4个库，二次分裂为8个，再16个。这样
      # 对于某个库的数据，一半数据移到新库，剩余不动，对比每次只增加一个库，所有数据都要大规模变动。
      # mod分库（基因拆分法）一般每个库记录数比较均匀，但也有些数据库，存在超级Id，这些Id的记录远远超过其他Id，比如在广告场景下，某个大广告主的广
      # 告数可能占总体很大比例。如果按照广告主Id取模分库，某些库的记录数会特别多，对于这些超级Id，需要提供单独库来存储记录。

   * 分库数量:
      - 分库数量首先和单库能处理的记录数有关，一般来说，Mysql 单库超过 5000万 条记录，Oracle单库超过 1亿 条记录，DB压力就很大(当然处理能力和字
        段数量/访问模式/记录长度有进一步关系)。
      - 在满足上述前提下，如果分库数量少，达不到分散存储和减轻DB性能压力的目的；如果分库的数量多，好处是每个库记录少，单库访问性能好，但对于跨多个
        库的访问，应用程序需要访问多个库，如果是并发模式(group,count,order,分页)，要消耗宝贵的线程资源；如果是串行模式(join)，执行时间会急剧增加。
      - 最后分库数量还直接影响硬件的投入，一般每个分库跑在单独物理机上，多一个库意味多一台设备。所以具体分多少个库，要综合评估，一般初次分库建议分
        4-8个库。

   * 分库分表扩容:
      扩容是很多分布式数据库、中间件心中的痛、好痛，如果是手动扩容，真是要死个人了。所以之前版本 mycat 都是推荐在分片是尽量预留好分片数量或者选择
      范围类、范围去摸类分片规则，本质上还是去尽量避免扩容操作
      * mycat
        - 2008阿里在去oracle中引入了Amoeba，相当于mysql的路由层：sql过滤、读写分离、负载均衡
        - 2012在Amoeba基础上升级开源了Cobar，10亿、3000台机器规模的集群，但开源后不再维护
        - 2013年基于Cobar系统的Mycat社区诞生，应用于互联网金融、电商。作为分布式中间层支持mysql、oracle、mongo等，
          提供功能:
          1.读写分离（数据库中间件最常用、最主要的功能），多从的时候支持读负载均衡。但不支持主节点宕机后自动选举的功能，即高可用
          2.垂直拆分。类似微服务中不同业务表的分类
          3.水平拆分。垂直之后、数据量大。支持一千万的表进行分片，支持单表最大一千亿的量级
          4.多租户场景（不同用户读取不同的表）
          5.数据统计（报表），多种类型的数据库
          管理mycat:
          因为实现了mysql协议，所以可以使用mysql客户端和mysql管理端口连接，通过show @@help;来查看所以的管理命令。



3: ID问题:
    一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的ID无法保证在全局上是唯一的；另一方
    面，应用程序在插入数据之前需要先获得ID,以便进行SQL路由.
    一些常见的主键生成策略:
      - UUID:
        使用UUID作主键是最简单的方案，但是缺点也是非常明显的。由于UUID非常的长，除占用大量存储空间外，最主要的问题是在索引上，在建立索引和基于索
        引进行查询时都存在性能问题。

      - 结合数据库维护一个Sequence表:
        此方案的思路也很简单，在数据库中建立一个Sequence表，表的结构类似于：
        CREATE TABLE `SEQUENCE` (  
            `table_name` varchar(18) NOT NULL,  
            `nextid` bigint(20) NOT NULL,  
            PRIMARY KEY (`table_name`)  
        ) ENGINE=InnoDB
        缺点：由于所有插入任何都需要访问该表，该表很容易成为系统性能瓶颈，同时它也存在单点问题，一旦该表数据库失效，整个应用程序将无法工作。
        有人提出使用Master-Slave进行主从同步，但这也只能解决单点问题，并不能解决读写比为1:1的访问压力问题。

      - Twitter的分布式自增ID算法Snowflake:
        在分布式系统中，需要生成全局UID的场合还是比较多的，twitter的snowflake解决了这种需求，实现也还是很简单的，除去配置信息，核心代码就是毫秒
        级时间41位 机器ID 10位 毫秒内序列12位。
        * 10---0000000000 0000000000 0000000000 0000000000 0 --- 00000 ---00000 ---000000000000
        第一位为未使用（实际上也可作为long的符号位），接下来的41位为毫秒级时间，然后5位datacenter标识位，5位机器ID（并不算标识符，实际是为线程
        标识），然后12位该毫秒内的当前毫秒内的计数，加起来刚好64位，为一个Long型。
        好处:整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞（由datacenter和机器ID作区分），并且效率较高，经测试，snowflake每秒能够
        产生26万ID左右，完全满足需要。


4: 事务:
    * 分布式事务:
      - 方案一：使用分布式事务
        优点：交由数据库管理，简单有效
        缺点：性能代价高，特别是shard越来越多时
      - 方案二：由应用程序和数据库共同控制
        原理：将一个跨多个数据库的分布式事务分拆成多个仅处 于单个数据库上面的小事务，并通过应用程序来总控 各个小事务。
        优点：性能上有优势
        缺点：需要应用程序在事务控制上做灵活设计。如果使用 了spring的事务管理，改动起来会面临一定的困难。
    * 事务补偿（幂等值）:
      - 对于那些对性能要求很高，但对一致性要求并不高的系统，往往并不苛求系统的实时一致性，只要在一个允许的时间周期内达
        到最终一致性即可，这使得事务补偿机制成为一种可行的方案。


5: 跨节点Join的问题:
    只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次
    查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。


6: 跨节点的count,order by,group by以及聚合函数问题:
    这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节
    点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以 并行 执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对
    应用程序内存的消耗是一个问题。


7: 跨分片的分页查询:
    - 当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片.
    - 当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，并将不同分片返回的结
      果集进行汇总和再次排序，最后再返回给用户。
      但是:
      如果想取出第10页数据，情况又将变得复杂很多，不能像获取第一页数据那样简单处理。因为第20页的10条数据很可能在某个节点的第11-20行，所以必须把每个
      节点的前 pageNum * pageSize 数据取出来，然后重新排序，所以越往后效率越低。解决方案：
      * 前台应用提供分页，则限定用户只能看前面n页，这个限制在业务上也是合理的，一般看后面的分页意义不大（如果一定要看，可以要求用户缩小范围重新查询）
      * 后台批处理任务要求分批获取数据，则可以加大page size，比如每次获取5000条记录，有效减少分页数（当然离线访问一般走备库，避免冲击主库）
        分库设计时，一般还有配套大数据平台汇总所有分库的记录，有些分页查询可以考虑走大数据平台。


8: 路由透明: 
    分库从某种意义上来说，意味着DB schema改变了，必然影响应用，但这种改变和业务无关，所以要尽量保证分库对应用代码透明，分库逻辑尽量在数据访问层处理
    。当然完全做到这一点很困难，具体哪些应该由DAL负责，哪些由应用负责，这里有一些建议：对于单库访问，比如查询条件指定用户Id，则该SQL只需访问特定库。
    此时应该由DAL层自动路由到特定库，当库二次分裂时，也只要修改mod 因子，应用代码不受影响。对于简单的多库查询，DAL负责汇总各个数据库返回的记录，此时
    仍对上层应用透明。
    参考中间件: Atlas, Cobar(淘宝), TDDL, kingshard(Go), IBatis Sharding, Hibernat Shards


9: 使用框架还是自主研发:
    总的来说，个人对于框架的选择是持谨慎态度的。一方面多数框架缺乏成功案例的验证，其成熟性与稳定性值得怀疑。另一方面，一些从成功商业产品开源出的框架
    （如阿里和淘宝的一些开源项目）是否适合你的项目是需要架构师深入调研分析的。当然，最终的选择一定是基于项目特点、团队状况、技术门槛和学习成本等综合
    因素考量确定的。







