年龄增长能带来更广阔的视野，也有助于形成自己的方法论，对于解决技术问题帮助颇多
套用xx的话来说就是：“很多坑一定要自己亲自踩过才有感觉。”

分布式事务: 
  分布式事务是指会涉及到操作多个数据库的事务。其实就是将对同一库事务的概念扩大到了对多个库的事务。目的是为了保证分布式系统中的数据一致性。
  分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（全部提交或全部回滚）

0: 幂等性:
  每次操作的结果一样（更新为100元／更新为120元问题）
     为保证事件的顺序一个简单的做法是在事件中添加时间戳，微服务记录每类型的事件最后处理的时间戳，如果收到的事件的时间戳早于
  我们记录的，丢弃该事件。如果事件不是在同一个服务器上发出的，那么服务器之间的时间同步是个难题，更稳妥的做法是使用一个全局递
  增序列号替换时间戳。

     对于本身不具有幂等性的操作，主要思想是为每条事件存储执行结果，当收到一条事件时我们需要根据事件的id查询该事件是否已经执
  行过，如果执行过直接返回上一次的执行结果，否则调度执行事件。
  重复处理开销大事件使用事件存储过滤重复事件:
    当收到一条事件时，过滤服务首先查询事件存储，确定该条事件是否已经被处理过，如果事件已经被处理过，直接返回存储的结果；否则
  调度业务服务执行处理，并将处理完的结果存储到事件存储中。在这个思想下我们需要考虑重复执行一条事件和查询存储结果的开销。
    如果重复处理一条事件开销很小，或者可预见只有非常少的事件会被重复接收，可以选择重复处理一次事件，在将事件数据持久化时由数
  据库抛出唯一性约束异常。
    如果重复处理一条事件的开销相比额外一次查询的开销要高很多，使用一个过滤服务来过滤重复的事件，过滤服务使用事件存储存储已经
  处理过的事件和结果。

  一般情况下上面的方法能够运行得很好，如果我们的微服务是RPC类的服务我们需要更加小心，可能出现的问题在于：
  （1）过滤服务在业务处理完成后才将事件结果存储到事件存储中，但是在业务处理完成前有可能就已经收到重复事件，由于是RPC服务也不
       能依赖数据库的唯一性约束；
  （2）业务服务的处理结果可能出现位置状态，一般出现在正常提交请求但是没有收到响应的时候。
    对于问题（1）可以按步骤记录事件处理过程，比如事件的记录事件的处理过程为“接收”、“发送请求”、“收到应答”、“处理完成”。好处
               是过滤服务能及时的发现重复事件，进一步还能根据事件状态作不同的处理。
    对于问题（2）可以通过一次额外的查询请求来确定事件的实际处理状态，要注意额外的查询会带来更长时间的延时，更进一步可能某些RPC
               服务根本不提供查询接口。此时只能选择接收暂时的不一致，时候采用对账和人工接入的方式来保证一致性。

1: 微服务架构下事务一致性: 实现了最终一致性
  1: 可靠事件模式: 
          这个过程可能导致出现不一致的地方在于：
          - 某个微服务在更新了业务实体后发布事件却失败；
          - 虽然微服务发布事件成功，但是消息代理未能正确推送事件到订阅的微服务；
          - 接受事件的微服务重复消费了事件。
      　　可靠事件模式在于保证可靠事件投递和避免重复消费，可靠事件投递定义为
          （a）每个服务原子性的业务操作和发布事件
          （b）消息代理确保事件传递至少一次。现在流行的消息队列都实现了事件的持久化和at least once的投递模式
          （b）消息代理确保事件投递至少一次已经满足，不做展开。
      本地事件表: 
              1.业务+事件映射记录持久化到DB，成功后commit
              2.同步发送消息
              3.事件恢复服务定时从事件表中恢复未发布成功的事件，重新发布，排除2失败的情况
      外部事件表: 把事件持久化到外部事件系统，同时事件系统还需要提供事件恢复服务来确认和恢复事件。
              1.业务+事件映射记录持久化到DB(只记录)，向事件系统发送事件
              2.向事件系统发送确认指令后事件系统才真正发布事件到消息代理
              3.如果业务回滚向事件系统发送回滚指令
              4.如果确认／回滚指令因为业务系统异常没有发送，事件系统主动查询业务+事件映射记录状态决定是否发送消息到消息中心
      幂等性: 
              参考0:幂等性


  2: 补偿模式: 
      　　补偿模式使用一个额外的协调服务来协调各个需要保证一致性的微服务，协调服务按顺序调用各个微服务，如果某个微服务调用异常（包括业务
      异常和技术异常）就取消之前所有已经调用成功的微服务。
　　      补偿模式建议仅用于不能避免出现业务异常的情况，如果有可能应该优化业务模式，以避免要求补偿事务。如账户余额不足的业务异常可通过预
      先冻结金额的方式避免，商品库存不足可要求商家准备额外的库存等。
          在补偿模式中一个比较明显的缺陷是，没有隔离性。从第一个工作服务步骤开始一直到所有工作服务完成（或者补偿过程完成），不一致是对其
      他服务可见的。另外最终一致性的保证还充分的依赖了协调服务的健壮性，如果协调服务异常，就没法达到一致性。
      
      重试策略:
          如果只是一味的失败就立即重试会给工作服务造成不必要的压力，我们要根据服务执行失败的原因来选择不同的重试策略。
          立即终止重试: 不是暂时性的，由于业务因素导致（如业务要素检查失败）的业务错误，这类错误是不会重发就能自动恢复的
          立即再次重试: 如果错误的原因是一些罕见的异常，比如因为网络传输过程出现数据丢失或者错误，因为类似的错误一般很少会再次发生。
          等待重试: 如果错误的原因是系统繁忙（比如http协议返回的500或者另外约定的返回码）或者超时,这个时候需要等待一些时间再重试。
          重试操作一般会指定重试次数上线，如果重试次数达到了上限就不再进行重试了。这个时候应该通过一种手段通知相关人员进行处理。
          对于等待重试的策略如果重试时仍然错误，可逐渐增加等待的时间，直到达到一个上限后，以上限作为等待时间。
          如果某个时刻聚集了大量需要重试的操作，补偿框架需要控制请求的流量，以防止对工作服务造成过大的压力。


  3: TCC模式（Try、Confirm、Cancel）: 
          需要注意的是第二阶段confirm或cancel操作本身也是满足最终一致性的过程，在调用confirm或cancel的时候也可能因为某种原因（比如网
      络）导致调用失败，所以需要活动管理支持重试的能力，同时这也就要求confirm和cancel操作具有幂等性。Confirm、Cancel操作满足幂等性。
          在TCC模式中直到明确的confirm动作，所有的业务操作都是隔离的（由业务层面保证）。另外工作服务可以通过指定try操作的超时时间，主动
      的cancel预留的业务资源，从而实现自治的微服务。TCC模式和补偿模式一样需要需要有协调服务和工作服务，协调服务也可以作为通用服务一般实
      现为框架。与补偿模式不同的是TCC服务框架不需要记录详细的业务流水，完成confirm和cancel操作的业务要素由业务服务提供。
          在confirm和cancel重试失败是，服务引入超时机制解决pengding的状态，区别于二阶段提交。

  4: 对账是最后的终极防线: 
　　      如果有些业务由于瞬时的网络故障或调用超时等问题，通过上文所讲的3种模式一般都能得到很好的解决。但是在当今云计算环境下，很多服务是依
      赖于外部系统的可用性情况，在一些重要的业务场景下还需要周期性的对账来保证真实的一致性。比如支付系统和银行之间每天日终是都会有对账过程。
  #https://yq.aliyun.com/articles/66109
  #http://mt.sohu.com/20160722/n460544753.shtml（简化版）

  XA协议:
    XA是一个分布式事务协议，由Tuxedo提出。XA中大致分为两部分：事务管理器和本地资源管理器。其中本地资源管理器往往由数据库实现，比如Oracle、
    DB2这些商业数据库都实现了XA接口，而事务管理器作为全局的调度者，负责各个本地资源的提交和回滚,总的来说，XA协议比较简单，而且一旦商业数据
    库实现了XA协议，使用分布式事务的成本也比较低。但是，XA也有致命的缺点，那就是性能不理想，特别是在交易下单链路，往往并发量很高，XA无法满足
    高并发场景。XA目前在商业数据库支持的比较理想，在mysql数据库中支持的不太理想，mysql的XA实现，没有记录prepare阶段日志，主备切换回导致主
    库与备库数据不一致。许多nosql也没有支持XA，这让XA的应用场景变得非常狭隘。

  补充:
    标准XA分布式事务由于性能太烂，遭到互联网同仁的一致性抛弃，大家纷纷造轮子，TCC啊消息队列等等。常见的一种思路：分布式事务拆开，比如先用本地
    事务保证先完成一部分，剩下用mq去做，可以重试，这个就需要应用保证幂等性了。还有一种先执行，出了异常进行补偿或者反向补偿。这些对开发者就苦了，
    要自己去保证幂等性等等，一个不小心就可能导致不一致情况出现。



2: CAP理论: 
  一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。
  Consistency 一致性: “all nodes see the same data at the same time”
  Availability 可用性: “Reads and writes always succeed”
  Partition Tolerance分区容错性: “the system continues to operate despite arbitrary message loss or failure of part of the system”


3: BASE理论: 
  BASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性（Strong Consistency，CAP的一致性就是强一致性），但应用可以采用适合的方式达到
  最终一致性（Eventual Consitency）。BASE是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency）。

  #http://www.ibm.com/developerworks/cn/websphere/library/techarticles/1601_clark-trs/1601_clark.html（逻辑简仓->内部服务组件化->微服务）


4: 二阶提交-三阶提交-Paxos算法-Raft算法: 二阶提交协议（Two Phase Commitment Protocol）、三阶提交协议（Three Phase Commitment Protocol）和Paxos算法
      #http://www.hollischuang.com/archives/681
  2PC:
      所谓的两个阶段是指：第一阶段：准备阶段(投票阶段)和第二阶段：提交阶段（执行阶段）。
      缺点: 
            1、同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。
            2、单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处
                于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参
                与者处于阻塞状态的问题）
            3、数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生
                了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到
                commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。
            4、二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了
                新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。
            由于二阶段提交存在着诸如同步阻塞、单点问题、脑裂等缺陷，所以，研究者们在二阶段提交的基础上做了改进，提出了三阶段提交
  3PC:
      引入超时机制，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。

      在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这
      个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是
      他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一
      句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。）

      缺点: 
        这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操
        作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。
  2PC与3PC的区别: 
      相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持
      有事务资源并处于阻塞状态。

  Paxos算法: 
      了解了2PC和3PC之后，我们可以发现，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。Google Chubby的作者
      Mike Burrows说过， there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken 
      versions of Paxos. 意即世上只有一种一致性算法，那就是Paxos，所有其他一致性算法都是Paxos算法的不完整版。后面的文章会介绍这个公认
      为难于理解但是行之有效的Paxos算法。

  Raft算法:
      简易版的Paxos算法


5: 三层: UI（用户展示层）、BLL（业务逻辑层）、DAL（数据访问层）
  DAL层: 
    读写分离，一个 Master节点对应多个 Salve 节点，将访问操作分摊至多个 Salve 节点上，解决单一数据库的负载压力。

    数据量大，引起CRUD操作的效率问题: 「垂直切分」------数据量再大------索引无法解决效率问题------>「水平切分」
      垂直切分: 根据业务自身的不同，将原本冗余在一个数据库内的业务表拆散、分别存储在不同的数据库中，同时仍然保持 Master/Salve模式。
      水平切分: 将一个业务表拆分成多个子表，比如 user_table0、user_table1、user_table2。子表通过某种契约关联在一起，按段位存储数据。
              拆分方法: 基因法，数据冗余法
  
      路由: 持久层需要判断出对应的数据源，以及数据源上的水平分区，这种访问方式我们称之为访问“路由”。
      按照常理来说，持久层不应该负责数据访问层(DAL)的工作，它应该只关心 one to one 的操作形式，所以淘宝的 TDDL 框架诞生也就顺其自然了。
      #http://www.tuicool.com/articles/nmeuu2

  分布式数据库中间件: Atlas, cobar, TDDL, kingshard(Go), IBatis Sharding, Hibernat Shards
    Atlas: 
      Atlas是由360公司Web平台部基础架构团队开发维护的一个基于MySQL协议的数据中间层项目。写主、读从；主库宕机，Atlas自动将宕机的主库摘
      除，写操作会失败，读操作不受影响。从库宕机，Atlas自动将宕机的从库摘除，对应用没有影响。自己实现读写分离，但写完马上就读而这时可能
      存在主从同步延迟的情况，Altas中可以在SQL语句前增加 /*master*/ 就可以将读请求强制发往主库。不能实现分布式分表，不能自动建表的功能。
      

    TDDL（Taobao Distributed Data Layer）:  
      解决分库分表场景下的访问路由（持久层与数据访问层的配合）以及异构数据库之间的数据同步 ，它是一个基于集中式配置的 JDBC DataSource 实现，
      具有分库分表、 Master/Salve 、动态数据源配置等功能，位于数据库和持久层之间（DBRoute）。

      依赖: 必须依赖 diamond 配置中心（ diamond 是淘宝内部使用的一个管理持久配置的系统，目前淘宝内部绝大多数系统的配置）。
      架构: Matrix 层、 Group 层和 Atom 层
          Matrix 层: 用于实现分库分表逻辑，底层持有多个 Group 实例。Group 层和 Atom 共同组成了 动态数据源.
          Group 层: 实现了数据库的 Master/Salve 模式的写分离逻辑，底层持有多个Atom 实例。
          Atom 层: (TAtomDataSource) 实现数据库ip,port,password,connectionProperties等信息的动态推送,以及持有原子的数据源分离的JBOSS数据源。
      原则: TDDL 透明给持久层的数据源接口应该是统一且“单一”的，至于数据库到底如何分库分表，持久层无需知道，也无需编写对应的SQL去实行应对策略。
      #http://www.guokr.com/blog/475765/
      #https://github.com/flike/kingshard/blob/master/README_ZH.md


6: 微服务拆分 #http://www.jianshu.com/p/80832f2d7a35
  拆分依据:
    微服务的理论知识有大量的分享，这里是我对微服务理论基础认识的一些观点：
    - 小，且专注于做一件事情，即满足单一职责原则。 关于单一职责可以阅读我的另一篇文章《软件开发中的单一职责》
    - 运行在独立的进程中。
    - 轻量级的通信机制，RPC或者HTTP或者MQ。
    - 松耦合，独立部署。
    - 康威定律：设计系统的组织，其产生的设计等同于组织之内、组织之间的沟通结构。

  服务拆分依据结合上面的理论基础充分考虑了以下因素:
    - 业务和领域模型
    - 技术、业务量等其他因素
    - 团队






