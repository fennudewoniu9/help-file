所有分布式几乎都用到的相似的设计: 分布式天下归一
  Chubby（Paxos算法）中的核心--多数同意；很多分布式架构 kaffka 等都用到了的Quorum机制
  Chubby服务端和客户端（包括客户端缓存），数据同步类似 Eureka 的服务端和客户端，通过轮训或者事件发布／订阅
  Chubby底层日志持久化类似 Hadoop 的Editlog和fsImage，方便宕机重启后快速恢复
  # Chubby是什么？参考下文中的【Paxos NB的应用1 -- Google Chubby: 】
  # paxos算法是Zookeeper的核心，Google Chubby架构师曾说，一切一致性协议都是paxos的变种。


******目录******
1: 分布式一致性
2: 一致性协议(算法)
3: ZooKeeper概要
4: ZooKeeper安装
5: Java API
6: 开源客户端
7: ZK典型应用场景
8: ZK技术内幕
9: 总结


1: 分布式一致性: 
  种类: 保证数据一致性会因为阻塞降低性能，提高性能必然会降低一致性，所以对于一个系统，性能和一致性需要平衡。一般一致性有三种：
    强一致性: 类似Kafka等Patition的leader和Replica之间的数据写确认（全部replica确认写成功的情况）
    弱一致性: 类似Mysql主从同步，会有一定延迟，不承诺具体多久，但一般是秒级。
    最终一致性: 类似通过kafka中间件解耦的业务数据在各模块之间一致性的情况。

  1.1: 分布式特点: 
    *对等性
        副本（Replica）: 是分布式系统中对数据和服务提供的一种冗余方式。
    *并发性
    *缺乏全局时钟
    *故障总会发生

  1.2: 分布式环境问题: 
    *通讯异常
        延迟: 现在计算机体系结构中，单机内存访问延迟在纳秒级（10ns左右），而正常的网络通讯延迟在0.1-1ms左右（105—106倍）
        副本（Replica）: 是分布式系统中对数据和服务提供的一种冗余方式。
    *网络分区
    *三态
        三态: 成功、失败、超时
    *节点故障

  1.3: 分布式理论: # ACID -> 分布式事务 -> CAP/BASE
    - ACID: 原子、一致（执行一半宕机已执行的数据不写入）、隔离（4种隔离级别-脏读／幻读）、持久
    - CAP: 2000年加州大学伯克利分校教授，一个分布式系统不可能同时满足一致性(C)、可用性(A)、分区容错性(P)
            可用性: 系统提供的服务在有限的时间内返回结果，类似 ZK(CP) 和 Eureka(AP) 理念的不同。
                  # google一般在0.3秒，HIVE一般20-30s（时间跨度较大时可能几分钟）
            分区容错性: 指在出现网络分区故障时，可以提供一致性、可用的服务
    - BASE: 最终一致性，eBay架构师提出。
            基本可用: 响应时间稍加增长、功能损失（部分购物者被引导到降级页面）
            软状态: 指中间状态、且其存在不影响整体可用，不同微服务数据同步存在延迟
            最终一致性: 

  1.4: 分布式提高性能措施: 
    副本复制
    Master选举


2: 一致性协议(算法): 
  2.1: 2PC: 提交事务请求、执行事务请求 
       简单、实现方便；
       同步阻塞、无限期等待（可以通过超时限制但可能会脑裂）、脑裂（协调节点的单点故障导致部分提交、部分不提交导致数据不一致）

  2.2: 3PC: CanCommit、PreCommit、DoCommit
       协调节点的单点故障后可以达成一致；
       网络分区后会造成数据不一致

  总结: 其实2PC和3PC都无法解决脑裂问题，即使是3PC，当第三阶段脑裂，而一部分server在第三阶段收到rollback指令，其余的默认还是会docommit，而Paxos
       中提出了一个超过半数的概念，就可以解决这个问题。 # Paxos中master角色（特殊的Proposer），只是具备提交提案的权利，最终通过还是需要半数以上。
  # 通常一个分布式一致性算法都需要用到一个特殊的机制：Quorum，指某一数据项值选定过程中需要集群中过半的机器达成一致。因此’Quorum机制‘也成‘过半机制’

  2.3: Paxos: 拜占庭将军问题有两个核心：消息被篡改、一致性，但一般分布式系统都是在局域网内，所以假设消息不会被篡改。Lamport在1990提出该算法，并
              在论文中设想了一个场景来解释原理：古希腊Paxos小岛上的议会投票。 # 因不愿修改论文被某杂志延迟发表，1996年麻省理工学院
              核心是一致性算法，解决的问题:
                - 被提出的提案才能被选中
                - 只能有一个值被选中
                - 如果某个进程认为一个提案被选中，那么该提案是所有人都认可的那个
              涉及到的对象:
                - Proposer：提议人
                - Acceptor：承兑人，Acceptor可以忽略任何请求（包括Prepare请求和Accept请求）而不用担心破坏算法的安全性
                - Learner：学习者，Acceptor中不参与投票的部分
                - Proposal：提案有一个编号和具体提案内容组成的[M,V]
              过程:
                - Proposer发送编号Mx，如果Acceptor接受的M小于Mx（比如Mx-1），就把M作为response返回给Proposer，且不再接受编号小于Mx的提案
                  # 向某个超过半数的子集成员发送
                - Proposer收到半数以上Mx返回，就发送[Mx,Vx]给所有Acceptor，任意一个Acceptor没有收到编号大于x的提案，就通过[Mx,Vx]的提案
                  # 收到的是所有Acceptor数量的一半以上，而不是发送子集的半数以上
                注：
                  *第一个阶段可能会发生死循环，所以只有主Proposer提议发起提案
                  *提案通过后Proposer发给部分Acceptor组成的集合，再由他们通知所有Learner
                  *只有master具备提案提交的资格，master选举参考Chubby

       Paxos NB的应用1 -- Google Chubby: 
                大名鼎鼎的分布式所服务，GFS（2003）、BigTable（2006）等大型系统都用它来解决分布式协作、元数据存储和master选举等和分布式锁相关
              的问题。最典型的应用场景就是集群服务中的master选举。
                Chubby是一个面向松耦合分布式系统的锁服务，通常用于为一个由大量小型计算机构成的松耦合分布式系统提供高可用的分布式锁服务。（一个分
              布式锁服务的目的是允许他的客户端进程同步彼此的操作，并对所处环境的基本信息达成一致），Chubby提供了粗粒度的服务，客户端接口设计的类
              似Unix的文件系统结构，可以操作文件节点（小文件）和文件节点的锁，而开发人员直接调用客户端接口即可实现进程间粗粒度的同步控制，从而保
              证分布式数据的一致性。Google Chubby并不是开源的，只有部分论文和文档，Yahoo借鉴了Chubby开发了Zookeeper

              # MapReduce很多人都知道，但关于Chubyy似乎熟悉它的就非常有限，这倒不奇怪，因为MapReduce是一个针对开发人员的ProgrammingModel，
              # 自然会有很多人去学习它，而Chubby更多的是一种为了实现MapReduce或者Bigtable而构建的内部的工具，对于开发人员来说基本上是透明的。
              
              # Chubby首先是一个分布式的文件系统。Chubby能够提供机制使得client可以在Chubby service上创建文件和执行一些文件的基本操作。说它是
              # 分布式的文件系统，是因为一个Chubby cell是一个分布式的系统，一般包含了5台机器，整个文件系统是部署在这5台机器上的。但是，从更高一
              # 点的语义层面上，Chubby是一个 lock service，一个针对松耦合的分布式系统的lock service。所谓lock service，就是这个service能够
              # 提供开发人员经常用的“锁”“解锁”功能。通过Chubby，一个分布式系统中的上千个client都能够对于某项资源进行“加锁”，“解锁”。那么，Chubby
              # 是怎样实现这样的“锁”功能的？就是通过文件。Chubby中的“锁”就是文件，在上例中，创建文件其实就是进行“加锁”操作，创建文件成功的那个
              # server其实就是抢占到了“锁”。用户通过打开、关闭和读取文件，获取共享锁或者独占锁；并且通过通信机制，向用户发送更新信息。综上所述，
              # Chubby是一个lock service，通过这个lock service可以解决分布式中的一致性问题，而这个lock service的实现是一个分布式的文件系统。
       
       Google Chubby应用场景／设计目标（侵入小、数据发布／订阅、高可靠高可用、粗粒度等）: 
              客户端一旦获取锁后会长时间持有(几个小时或者几天)，即使短暂宕机也不会丢失持有状态，这也就是所说的粗粒度。master获取锁后，会把master
              信息写入该文件节点，其他节点失去该节点的写权限，但是可以读取该节点，以获取master信息。可以支撑成百上千个客户端节点的监视、读取。服务
              端的信息变化是通过事件的形式通知所有订阅的客户端，而不是通过客户端不断轮训。
       
       Google Chubby技术架构（锁序列器/锁延迟、KeepAlive、会话超时、master故障恢复等）: 
              服务端和客户端组成，客户端通过RPC调用与服务端通讯。一般使用5台机器构成一个集群，通过Paxos协议选举出一个master，在一个租期内Chubby
              会保证该机器是唯一的master，到期后可以续租。如果Master对应的机器宕机，租期到期后会重新选举，一般花费几秒钟。
              客户端向服务端注册事件通知，如果服务端节点发生变化，进行回调，类似EurekaServer和EurekaClient，且更相似的是，在ChubbyClient会有
              文件内容和原数据信息的缓存。（服务端和客户端缓存一致性策略：客户端租期(一般12s) + 服务端变更通知）
              会话:
                客户端和服务端通过TCP连接+心跳机制保持会话的活性，以使会话周期得到延续。
              层级结构:
                容错日志系统：低层，通过Poxos选举Master进行日志复制
                容错数据库：中层，利用BTree进行日志存储 # 采用类似hadoop的editLog和fsImage来定期清理、存储日志到磁盘，宕机重启后可迅速恢复数据
                Chubby服务：最上层，提供分布式锁和小文件存储服务
              master故障恢复:
                依靠 宽限期 实现Chubby Master的故障恢复，客户端和服务端依靠 租期+宽限期 实现故障的平滑恢复。

       Paxos NB的应用2 -- Hypertable: 
                根据Google的BigTable相关论文，使用C++开发的开源、高性能、可伸缩的数据库，与Hbase类似，用于构建一个分布式海量数据的高并发数据库，
              不支持事务、关联查询，少量数据时查询效率不如传统的关系型数据库，但支持高并发、海量、随意扩容、节点失效后高可用。


3: ZooKeeper概要: 
    2011年成为Apache顶级项目，从Apache Hadoop子项目发展来的，雅虎创建，是Google Chubby的开源实现。提供高效、可靠的分布式协调服务，如统一命名、
  配置管理、分布式锁、发布／订阅、负载均衡、master选举、分布式队列。没有采用Paxos协议，用的是 ZAB 一致性协议（ZooKeeper Atomic Broadcast）。

  定义:
    ZooKeeper是一个高可用的分布式数据管理与协调框架，通讯协议是基于TCP/IP的自定义协议。参考8技术内幕。
    进程: 使用一个单一的主进程接收、处理客户端发来的事务请求，并采用ZAB原子广播协议，将服务器的数据变更状态以事务Proposal的形式广播多所有副本进程。
    特性: 顺序一致性、原子性、单一视图、可靠性、实时性
    节点: 数据节点组成了类似文件系统的数据模型

  三个角色: Leader、Follower、Observer #Leader的存在是为了提升决议效率 
      * 观察者不参与选举和半数以上策略
      * 非Leader接收到请求后会转发给Leader
      * Leader会为每个Follower准备一个队列用于控制广播
      * Follower收到请求后会把数据写到磁盘而不是只是内存，然后再向Leader发送ACK
      * Follower、Observer:对于非事务请求可以自己处理，对于事务请求转发给Leader处理。作用：不影响事务请求处理能力的前提下，提高非事务处理能力。
      * Observer和Follower的唯一区别是不参与任何形式的投票，包括事务请求和leader选举投票

  特点: 
      * 集群: 一般3-5台组成一个集群
      * 顺序读写: 通过选举唯一的ZXID实现顺序访问
      * 高性能: 适合以读为主要操作的应用场景
      * 会话: 通过TCP连接，存在一个类似Chubby的“宽限期”，支持客户端重连
      * 版本: 每个ZNode都有，存在stat数据结构中（命令：stat ／path），记录ZNode、ZNode子节点、ACL数据
      * Watcher: 允许客户端注册事件，在事件被触发时服务端会推送到客户端 # 一次性、需要反复注册.弊端:可能会引起羊群效应(过多无关的通知)
      * ACL: 类似Unix的权限控制系统

  ZAB协议: 恢复崩溃、原子广播协议，比Paxos多了恢复崩溃。保证ZK任何时候都只有一个主进程进行消息广播，如果其崩溃了，进行重新选举。
      不像Paxos通用，专门为zookeeper设计的崩溃可恢复的原子消息广播算法。ZAB类似2PC，但是简化了（在收到半数而不是全部client的ACK后及发送commit）

      * ZAB两种模式: 恢复模式、消息广播模式（TCP协议） 
        # 崩溃恢复包括master崩溃和新节点加入两种情况。对提案的处理会有提交和丢弃两种。
          崩溃恢复 + 消息广播: 发现／选举（ZXID最大的机器）、同步（Follower根据Leader的情况丢弃／提交相关提案）、广播 
                             # 正常情况下zk会一直处于阶段三，特殊情况会进行1、2、3的循环
                             # 只有阶段2完成后，才会进入阶段3
          对应的每个进程都会是三个中的一种: # 三种状态会随着以上三种阶段的变化而变化
              Looking: Leader选举状态（运行后选举时会优先使用epoch编号最大的节点进行Leader的竞选）
              FOLLOWING: Follower和Leader保持同步状态
              LEADING: Leader作为主进程领导状态

      * 逻辑时钟epoch: 选举的轮数，随着选举的轮数++

      * Zk选举算法: 一种是基于basic paxos实现，一种是基于fast paxos算法实现。系统默认的是fast paxos。 
                   basic paxos: 最先发起的线程担任，首先推荐自己，然后根据response推荐zxid最大的，如果相同推荐myid最大的作为leader
                   fast paxos: 所有节点均把自己的epoch、zxid、myid广播(类似paxos)推荐自己，根据response的状态(looking或者
                               following／leading两种情况)和zxid、myid这三者的顺序判断谁是leader、或者继续下一个epoch。
                   共同点: zxid最大的当选

      * 事务编号ZXID: 高32位是Leader周期epoch编号，低32位是简单的递增计数器（客户端发送请求后递增），Leader、Follower、Observer都存。

      * 新节点加入: 新节点进入数据恢复模式，然后找到Leader所在机器，与其进行数据同步，然后进入数据参与到数据广播流程中。

      * 和Paxos的区别: 多了一个同步阶段。发现类似Paxos的提案提交竞选，广播类似提案的广播提交。同步阶段可以保证Leader在处理client请求提出新的
                      Proposal（提案）之前，所有的进群节点（所有的集群进程）都完成了之前所有已提交Proposal的提交。
                      ZAB主要用于构建一个高可用、分布式、主备、数据系统 #参考”定义“
                      Paxos主要用于构建一个高可用、分布式、一致性、状态机系统
                 总结：ZAB强调过去、现在的数据在主备上都要一致，Paxos强调现在（提案提交发生的那一次）各个节点的一致。 #参考同步阶段做的事情。


4: ZooKeeper安装: 
  模式: 集群、单机
  配置: 
        每个机器都把zoo_sample.cfg修改成zoo.cfg，然后修改以下几项配置，在dataDir下需要创建一个myid文件，只有一个数字（server.id，1—255）
        # The number of milliseconds of each tick
        tickTime=2000
        # The number of ticks that the initial
        # synchronization phase can take
        initLimit=10
        # The number of ticks that can pass between
        # sending a request and getting an acknowledgement
        syncLimit=5
        # the directory where the snapshot is stored.
        # do not use /tmp for storage, /tmp here is just
        # example sakes.
        dataDir=/app/data/zookeeper
        # the port at which the clients will connect
        clientPort=2181
        # the maximum number of client connections.
        # increase this if you need to handle more clients
        #maxClientCnxns=60
        #
        # Be sure to read the maintenance section of the
        # administrator guide before turning on autopurge.
        #
        # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
        #
        # The number of snapshots to retain in dataDir
        #autopurge.snapRetainCount=3
        # Purge task interval in hours
        # Set to "0" to disable auto purge feature
        #autopurge.purgeInterval=1
        server.1=10.213.129.58:2999:3999 #如果是单机模式，只要配置server.1即可
        server.2=10.213.129.59:2999:3999
        server.3=10.213.128.98:2999:3999

  启动: 
    ./zkServer.sh start # ./zkServer.sh {start|start-foreground|stop|restart|status|upgrade|print-cmd}

  验证: 
    1.telnet localhost  2181
    2.在出来的界面中直接输入stat
    # Trying 127.0.0.1...
    # Connected to localhost.
    # Escape character is '^]'.
    # stat
    # Zookeeper version: 3.4.9-1757313, built on 08/23/2016 06:50 GMT
    # Clients:
    #  /10.213.129.58:50958[1](queued=0,recved=67531,sent=67541)
    #  /127.0.0.1:40790[0](queued=0,recved=1,sent=0)
    #  /10.213.129.59:34216[1](queued=0,recved=67344,sent=67344)
    #  /10.213.129.60:60482[1](queued=0,recved=67380,sent=67380)
    #  /10.213.128.98:23202[1](queued=0,recved=340368,sent=394371)
    #  /10.213.129.61:50856[1](queued=0,recved=10095,sent=10095)
    # Latency min/avg/max: 0/0/68
    # Received: 561873
    # Sent: 615885
    # Connections: 6
    # Outstanding: 0
    # Zxid: 0xa00009125
    # Mode: follower
    # Node count: 1779
    # Connection closed by foreign host.
    注: 常用的四字命令：
                      stat: 服务端运行时状态信息 # zk版本、运行时角色、集群节点个数、当前服务器的客户端连接信息等
                      conf: 服务端运行时配置信息
                      envi: 服务端运行时环境配置信息 # java verion等
                      cons: 当前这个机器上所有客户端连接的信息
                      ruok: 检测当前zk服务器是否正常运行 # 读音类似Are you ok，输出：imok
                      wchs: 输出当前服务器上管理的watcher概要信息
                      wchc: 输出当前服务器上管理的watcher详细信息
                      mntr: 比stat更详细的信息

  命令: 
    * 运行客户端: ./zkCli.sh # 这个默认是连接本地的，连接远程的：./zkCli.sh -server ip:port
    * 看到目录: ls
    * 看文结点: get
    PS: 随便出入一个错误的单词就会列出所有支持的命令

  规定: 
    * 所有非叶子节点的节点必须为持久节点



5: Java API:  # 客户端版本要和服务端保持一致
  * 创建session: zookeeper会话的建立是一个异步过程，需要等待一段时间(用CountDownLatch实现)，只有服务端发送事件通知，才算创建成功
  * 节点不支持递归创建: 即无法在夫节点不存的情况下创建子节点，且如果子节点存在会报错（NodeExsitsException）
  * ZooKeeper不负责节点内容序列化: 需要自己是用Hession等序列化工具进行序列化、反序列化
  * 节点创建有同步、异步两种方式: 同步可能扔异常，异步的错误不是通过异常，而是通过返回码体现的
  * 读取节点数据: 没有版本选项，版本主要是为了解决分布式并发问题
  * 更新节点数据: 可以制定版本进行更新（从0开始），如果用-1代表对版本没要求，直接更新最新的版本；该机制可以避免分布式并发问题
  * 更新节点数据: 节点注册了事件，但是对于子节点的变化，不会通知客户端
  * 节点权限: 节点加权限后对该节点的读取控制，有效范围是本节点；但是删除控制，有效范围是子节点，即本节点即使不加权限信息依然可以删除


6: 开源客户端: 
  ZKClient: Stars：700+  # 客户端版本要和服务端保持一致
    Github开源组件，实现了Session超时重连、Watcher反复注册等功能。与原生的API的区别：
    * 创建session: 创建session时把原声的异步封装成同步的
    * 读取节点: ZKClient获取节点内容时候自动反序列化
    * Watcher: 用习惯的Listener实现，而且Listener里的返回值自动把原生的 获取节点变化事件、请求最新子节点列表 这两步操作合成为一步
               Listener不是一次性的，一次注册永久有效

  Curator:  Stars：1000+ # 客户端版本要和服务端保持一致
    Netflix的开源Zeekooper客户端框架，实现了连接重连、反复注册Watcher、NodeExsitsException等，且提供Fluent风格的API接口，已是Apache顶级项目，
    zk使用最广泛的客户端。 # Guava is to Java what Curator is to ZooKeeper
    * 创建session: 比原生的河ZKClient更灵活，增加了重试策略和Fluent风格的API接口
    * 创建节点: 实现了自动递归创建父节点，避免了每次创建前都判断父节点是否存在；如果节点存在不会报错，而是通过返回码体现
    * 删除节点: 提供强制删除功能，底层通过客户端记录失败信息，重试。master选举是通过删除、新增节点实现的，这种场景下网络问题很致命
    * 读取节点: 提供了读取数据同时还能获取stat的信息
    * 回调: 提供BackgroundCallback接口，用来处理调用异步接口之后服务端返回的结果（CuratorEventType，码-事件类型）
    * Watcher: 用cache实现，cache是对事件进行监听

    Curator典型应用场景:
      * master选举: 需要引入额外的jar，实现接口LeaderSelector和LeaderSelectorListenerAdapter
      * 分布式锁: 实现接口InterProcessMutex
      * 分布式计数器: 实现接口DistributedAtomicInteger
      * 分布式Barrier: Barrier是多线程之间同步的经典访问模式；功能同时触发所有等待的线程同时执行各自的业务逻辑。
                      单点：不同进程可以用JDK自带的CyclicBarrier，同一个进程不同的线程可以用CountDownLatch  
                      分布式：Curator实现接口DistributedBarrier

    Curator常用的工具类: #不需要引入额外的jar包
      * ZKPaths: 路径递归创建、删除等功能
      * EnsurePath: 确保节点存在（一定会创建成功且不会报异常）
      * TestingServer: 没有zk服务器但是需要单元测试时，模拟一个标准的zk服务器，端口、datadir都可以执行。datadir如果不指定会用java.io.tmpdir
      * TestingCluster: 类似TestingServer，TestingCluster模拟的是集群。


7: ZK典型应用场景: 
  * 数据发布／订阅
  * 复杂均衡
  * 分布式协调／通知 # 类似IgniteElasticJob，三个核心模块
  * 集群管理
  * master选举 # watcher和莅临节点，可以避免数据库master选举出现的master节点挂了之后其他客户端无法感知的问题
  * 分布式锁
  * 分布式队列 # 原理：临时顺序节点。Barrier
  
8: ZK技术内幕:
  * Packet: 服务端和客户端最小的通讯单元：Packet
  * Watcher: 所有注册到服务端的Watcher会保存在客户端的ZKWatcherManager
  * 序列化／反序列化: Jute（核心类Record）
  * 通讯协议: 基于TCP/IP实现的自定义通讯协议（三部分：长度、请求头、请求体） # Wireshark：网络封包分析软件

9: 总结
  2PC与3PC的区别: 3降低了阻塞范围，提高了一致性的概率，但是在分区企且需要abrot时，都没有最终解决一致性的问题。
  3PC与Paxos的区别: Paxos引入了Quorum机制（过半写成功概念），解决了分区一致性的问题。
  Paxos与拜占庭将军问题: Paxos没有解决这个历史问题，因为Paxos只解决了一致性问题，而没有解决信道传输数据安全的问题。
  Paxos与ZAB区别: ZAB主要用于构建一个高可用、分布式、主备、数据系统 #参考”定义“
                 Paxos主要用于构建一个高可用、分布式、一致性、状态机系统
                 总结：ZAB强调过去、现在的数据在主备上都要一致，Paxos强调现在（提案提交发生的那一次）各个节点的一致。 #参考同步阶段做的事情。
                      因为ZAB三阶段（发现／选主、同步、处理）中的同步阶段是在zk三层结构基础上进行数据的转移、复制。



